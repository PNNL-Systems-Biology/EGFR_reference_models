{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test using Pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer import MCMC, NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0010005001667084"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.e ** 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.9749)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    k1 = pyro.sample('k1', dist.Normal(np.log10(0.0016), 0.001))\n",
    "    k_1 = pyro.sample('k_1', dist.Normal(np.log10(0.004), 0.001))\n",
    "    kt = pyro.sample('kt', dist.Normal(np.log10(0.0012), 0.001))\n",
    "    ke = pyro.sample('ke', dist.Normal(np.log10(0.0033), 0.001))\n",
    "    Vr = pyro.sample('Vr', dist.Normal(np.log10(0.0012), 0.001))\n",
    "    kx = pyro.sample('kx', dist.Normal(np.log10(1), 0.001))\n",
    "    kh = pyro.sample('kh', dist.Normal(np.log10(0.0004), 0.001))\n",
    "    k2 = pyro.sample('k2', dist.Normal(np.log10(0.082), 0.001))\n",
    "    k_2 = pyro.sample('k_2', dist.Uniform(-4, 2))\n",
    "    k3 = pyro.sample('k3', dist.Normal(np.log10(1.2), 0.001))\n",
    "    k_3 = pyro.sample('k_3', dist.Uniform(-4, 2))\n",
    "    res = getResidual(model, np.array(10 ** np.array([k1.item(), k_1.item(), kt.item(), ke.item(), Vr.item(), \n",
    "                                       kx.item(), kh.item(), k2.item(), k_2.item(), k3.item(), k_3.item()])))\n",
    "    pyro.sample(\"obs\", dist.Normal(res, 0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9749])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    k1 = pyro.sample('k1', dist.Normal(np.log10(0.0016), 0.001))\n",
    "    k_1 = pyro.sample('k_1', dist.Normal(np.log10(0.004), 0.001))\n",
    "    kt = pyro.sample('kt', dist.Normal(np.log10(0.0012), 0.001))\n",
    "    ke = pyro.sample('ke', dist.Normal(np.log10(0.0033), 0.001))\n",
    "    Vr = pyro.sample('Vr', dist.Normal(np.log10(0.0012), 0.001))\n",
    "    kx = pyro.sample('kx', dist.Normal(np.log10(1), 0.001))\n",
    "    kh = pyro.sample('kh', dist.Normal(np.log10(0.0004), 0.001))\n",
    "    k2 = pyro.sample('k2', dist.Normal(np.log10(0.082), 0.001))\n",
    "    k_2 = pyro.sample('k_2', dist.Uniform(-4, 2))\n",
    "    k3 = pyro.sample('k3', dist.Normal(np.log10(1.2), 0.001))\n",
    "    k_3 = pyro.sample('k_3', dist.Uniform(-4, 2))\n",
    "\n",
    "    res = getResidual(model, 10 ** np.array([k1.item(), k_1.item(), kt.item(), ke.item(), Vr.item(), \n",
    "                                       kx.item(), kh.item(), k2.item(), k_2.item(), k3.item(), k_3.item()]))\n",
    "    torch.ones(1) * res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resModel(model):\n",
    "    k1 = pyro.sample('k1', dist.Normal(np.log10(0.0016), 0.001))\n",
    "    k_1 = pyro.sample('k_1', dist.Normal(np.log10(0.004), 0.001))\n",
    "    kt = pyro.sample('kt', dist.Normal(np.log10(0.0012), 0.001))\n",
    "    ke = pyro.sample('ke', dist.Normal(np.log10(0.0033), 0.001))\n",
    "    Vr = pyro.sample('Vr', dist.Normal(np.log10(0.0012), 0.001))\n",
    "    kx = pyro.sample('kx', dist.Normal(np.log10(1), 0.001))\n",
    "    kh = pyro.sample('kh', dist.Normal(np.log10(0.0004), 0.001))\n",
    "    k2 = pyro.sample('k2', dist.Normal(np.log10(0.082), 0.001))\n",
    "    k_2 = pyro.sample('k_2', dist.Uniform(-4, 2))\n",
    "    k3 = pyro.sample('k3', dist.Normal(np.log10(1.2), 0.001))\n",
    "    k_3 = pyro.sample('k_3', dist.Uniform(-4, 2))\n",
    "\n",
    "    res = getResidual(model, 10 ** np.array([k1.item(), k_1.item(), kt.item(), ke.item(), Vr.item(), \n",
    "                                       kx.item(), kh.item(), k2.item(), k_2.item(), k3.item(), k_3.item()]))\n",
    "\n",
    "    return torch.ones(1) * res\n",
    "\n",
    "def conditioned_model(resModel, model, y):\n",
    "    return poutine.condition(resModel, data={\"obs\": y})(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_kernel = NUTS(conditioned_model, jit_compile=False)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=1000, num_chains=4)\n",
    "mcmc.run(resModel, model, torch.zeros(1))\n",
    "mcmc.summary(prob=0.68)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample tensor(-1.3905)\n",
      "log prob tensor(-1.8857)\n"
     ]
    }
   ],
   "source": [
    "loc = 0.   # mean zero\n",
    "scale = 1. # unit variance\n",
    "normal = torch.distributions.Normal(loc, scale) # create a normal distribution object\n",
    "x = normal.rsample() # draw a sample from N(0,1)\n",
    "print(\"sample\", x)\n",
    "print(\"log prob\", normal.log_prob(x)) # score the sample from N(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather():\n",
    "    cloudy = torch.distributions.Bernoulli(0.3).sample()\n",
    "    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n",
    "    mean_temp = {'cloudy': 55.0, 'sunny': 75.0}[cloudy]\n",
    "    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n",
    "    temp = torch.distributions.Normal(mean_temp, scale_temp).rsample()\n",
    "    return cloudy, temp.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.8152)\n"
     ]
    }
   ],
   "source": [
    "x = pyro.sample(\"my_sample\", pyro.distributions.Normal(loc, scale))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather():\n",
    "    cloudy = pyro.sample('cloudy', pyro.distributions.Bernoulli(0.3))\n",
    "    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n",
    "    mean_temp = {'cloudy': 55.0, 'sunny': 75.0}[cloudy]\n",
    "    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n",
    "    temp = pyro.sample('temp', pyro.distributions.Normal(mean_temp, scale_temp))\n",
    "    return cloudy, temp.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cloudy', 64.5440444946289)\n",
      "('sunny', 94.37557983398438)\n",
      "('sunny', 72.5186767578125)\n"
     ]
    }
   ],
   "source": [
    "def weather():\n",
    "    cloudy = pyro.sample('cloudy', pyro.distributions.Bernoulli(0.3))\n",
    "    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n",
    "    mean_temp = {'cloudy': 55.0, 'sunny': 75.0}[cloudy]\n",
    "    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n",
    "    temp = pyro.sample('temp', pyro.distributions.Normal(mean_temp, scale_temp))\n",
    "    return cloudy, temp.item()\n",
    "\n",
    "for _ in range(3):\n",
    "    print(weather())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ice_cream_sales():\n",
    "    cloudy, temp = weather()\n",
    "    expected_sales = 200. if cloudy == 'sunny' and temp > 80.0 else 50.\n",
    "    ice_cream = pyro.sample('ice_cream', pyro.distributions.Normal(expected_sales, 10.0))\n",
    "    return ice_cream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def geometric(p, t=None):\n",
    "    if t is None:\n",
    "        t = 0\n",
    "    x = pyro.sample(\"x_{}\".format(t), pyro.distributions.Bernoulli(p))\n",
    "    if x.item() == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 + geometric(p, t + 1)\n",
    "\n",
    "print(geometric(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1493)\n"
     ]
    }
   ],
   "source": [
    "def normal_product(loc, scale):\n",
    "    z1 = pyro.sample(\"z1\", pyro.distributions.Normal(loc, scale))\n",
    "    z2 = pyro.sample(\"z2\", pyro.distributions.Normal(loc, scale))\n",
    "    y = z1 * z2\n",
    "    return y\n",
    "\n",
    "def make_normal_normal():\n",
    "    mu_latent = pyro.sample(\"mu_latent\", pyro.distributions.Normal(0, 1))\n",
    "    fn = lambda scale: normal_product(mu_latent, scale)\n",
    "    return fn\n",
    "\n",
    "print(make_normal_normal()(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer import MCMC, NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resModel():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5410, -0.2934, -2.1788,  0.5684, -1.0845, -1.3986,  0.4033,  0.8380,\n",
       "        -0.7193, -0.4033])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.sample('eta', dist.Normal(torch.zeros(10), torch.ones(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.9664])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.sample('mu', dist.Normal(torch.zeros(1), 10 * torch.ones(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.2214])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.sample('tau', dist.HalfCauchy(scale=25 * torch.ones(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = pyro.sample('eta', dist.Normal(torch.zeros(10), torch.ones(10)))\n",
    "mu = pyro.sample('mu', dist.Normal(torch.zeros(1), 10 * torch.ones(1)))\n",
    "tau = pyro.sample('tau', dist.HalfCauchy(scale=25 * torch.ones(1)))\n",
    "\n",
    "theta = mu + tau * eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-21.0833, -14.9126, -29.6163, -26.7419, -22.3080, -26.3238, -19.2022,\n",
       "        -29.7435, -21.0826, -19.0633])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-20.8243, -16.6829, -29.4021, -27.2801, -21.7200, -24.7179, -18.7743,\n",
       "        -30.4211, -20.0404, -21.0145])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.sample(\"obs\", dist.Normal(theta, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 59.7300,  20.8363, -16.3221,  38.9358,  10.9666,   1.6052,  -6.2763,\n",
       "        -15.5909, -61.4017,  18.0785])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    eta = pyro.sample('eta', dist.Normal(torch.zeros(10), torch.ones(10)))\n",
    "    mu = pyro.sample('mu', dist.Normal(torch.zeros(1), 10 * torch.ones(1)))\n",
    "    tau = pyro.sample('tau', dist.HalfCauchy(scale=25 * torch.ones(1)))\n",
    "\n",
    "    theta = mu + tau * eta\n",
    "    theta\n",
    "    #pyro.sample(\"obs\", dist.Normal(theta, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479aa1f143fb4644925e58303f1c306a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Warmup [1]', max=2000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c54ceb903d4bd0a85f56d6cc5f5236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Warmup [2]', max=2000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad499e07315463e9ed274fec0a55a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Warmup [3]', max=2000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fea7d9de4e64daeb786404ecc4f298f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Warmup [4]', max=2000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                mean       std    median     25.0%     75.0%     n_eff     r_hat\n",
      "    eta[0]      0.00      0.93     -0.02     -0.70      0.50   4186.91      1.00\n",
      "    eta[1]     -0.00      0.95     -0.00     -0.58      0.69   4149.91      1.00\n",
      "    eta[2]     -0.01      0.93     -0.00     -0.51      0.77   4440.24      1.00\n",
      "    eta[3]      0.01      0.96     -0.02     -0.60      0.67   3823.44      1.00\n",
      "    eta[4]      0.00      0.94     -0.01     -0.67      0.58   4654.07      1.00\n",
      "    eta[5]      0.00      0.96      0.02     -0.62      0.65   4793.30      1.00\n",
      "    eta[6]      0.01      0.98     -0.03     -0.65      0.65   4281.17      1.00\n",
      "    eta[7]     -0.02      0.95     -0.01     -0.64      0.61   4875.79      1.00\n",
      "    eta[8]     -0.00      0.95     -0.01     -0.62      0.66   4226.83      1.00\n",
      "    eta[9]      0.00      0.98      0.00     -0.66      0.64   4891.93      1.00\n",
      "     mu[0]      1.00      0.33      0.99      0.77      1.22   4272.22      1.00\n",
      "    tau[0]      0.31      0.25      0.25      0.00      0.25   2995.34      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "pyro.enable_validation(__debug__)\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "\n",
    "def model(sigma):\n",
    "    eta = pyro.sample('eta', dist.Normal(torch.zeros(10), torch.ones(10)))\n",
    "    mu = pyro.sample('mu', dist.Normal(torch.zeros(1), 10 * torch.ones(1)))\n",
    "    tau = pyro.sample('tau', dist.HalfCauchy(scale=25 * torch.ones(1)))\n",
    "\n",
    "    theta = mu + tau * eta\n",
    "\n",
    "    return pyro.sample(\"obs\", dist.Normal(theta, sigma))\n",
    "\n",
    "\n",
    "def conditioned_model(model, sigma, y):\n",
    "    return poutine.condition(model, data={\"obs\": y})(sigma)\n",
    "\n",
    "\n",
    "\n",
    "nuts_kernel = NUTS(conditioned_model, jit_compile=False)\n",
    "mcmc = MCMC(nuts_kernel,\n",
    "            num_samples=1000,\n",
    "            warmup_steps=1000,\n",
    "            num_chains=4)\n",
    "mcmc.run(model, 1.0, torch.ones(10))\n",
    "mcmc.summary(prob=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median     16.0%     84.0%     n_eff     r_hat\n",
      "    eta[0]      0.00      0.93     -0.02     -0.84      0.94   4186.91      1.00\n",
      "    eta[1]     -0.00      0.95     -0.00     -1.05      0.81   4149.91      1.00\n",
      "    eta[2]     -0.01      0.93     -0.00     -0.96      0.89   4440.24      1.00\n",
      "    eta[3]      0.01      0.96     -0.02     -0.97      0.93   3823.44      1.00\n",
      "    eta[4]      0.00      0.94     -0.01     -0.93      0.91   4654.07      1.00\n",
      "    eta[5]      0.00      0.96      0.02     -0.83      1.04   4793.30      1.00\n",
      "    eta[6]      0.01      0.98     -0.03     -0.96      0.96   4281.17      1.00\n",
      "    eta[7]     -0.02      0.95     -0.01     -0.94      0.94   4875.79      1.00\n",
      "    eta[8]     -0.00      0.95     -0.01     -1.02      0.87   4226.83      1.00\n",
      "    eta[9]      0.00      0.98      0.00     -1.02      0.90   4891.93      1.00\n",
      "     mu[0]      1.00      0.33      0.99      0.62      1.27   4272.22      1.00\n",
      "    tau[0]      0.31      0.25      0.25      0.00      0.38   2995.34      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "mcmc.summary(prob=0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal(loc: 0.0, scale: 0.10000000149011612)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.Normal(0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mMCMC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Prints a summary table displaying diagnostics of samples obtained from\n",
       "posterior. The diagnostics displayed are mean, standard deviation, median,\n",
       "the 90% Credibility Interval, :func:`~pyro.ops.stats.effective_sample_size`,\n",
       ":func:`~pyro.ops.stats.split_gelman_rubin`.\n",
       "\n",
       ":param float prob: the probability mass of samples within the credibility interval.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/syssynbio/lib/python3.7/site-packages/pyro/infer/mcmc/api.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?MCMC.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poutine.condition(model, data={\"obs\": torch.ones(10) * 2})(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mNUTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpotential_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madapt_step_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madapt_mass_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfull_mass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_multinomial_sampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_plate_nesting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mjit_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mjit_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_jit_warnings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget_accept_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_tree_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minit_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0minit_to_uniform\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7fea683b2560\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "No-U-Turn Sampler kernel, which provides an efficient and convenient way\n",
       "to run Hamiltonian Monte Carlo. The number of steps taken by the\n",
       "integrator is dynamically adjusted on each call to ``sample`` to ensure\n",
       "an optimal length for the Hamiltonian trajectory [1]. As such, the samples\n",
       "generated will typically have lower autocorrelation than those generated\n",
       "by the :class:`~pyro.infer.mcmc.HMC` kernel. Optionally, the NUTS kernel\n",
       "also provides the ability to adapt step size during the warmup phase.\n",
       "\n",
       "Refer to the `baseball example <https://github.com/pyro-ppl/pyro/blob/dev/examples/baseball.py>`_\n",
       "to see how to do Bayesian inference in Pyro using NUTS.\n",
       "\n",
       "**References**\n",
       "\n",
       "[1] `The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo`,\n",
       "    Matthew D. Hoffman, and Andrew Gelman.\n",
       "[2] `A Conceptual Introduction to Hamiltonian Monte Carlo`,\n",
       "    Michael Betancourt\n",
       "[3] `Slice Sampling`,\n",
       "    Radford M. Neal\n",
       "\n",
       ":param model: Python callable containing Pyro primitives.\n",
       ":param potential_fn: Python callable calculating potential energy with input\n",
       "    is a dict of real support parameters.\n",
       ":param float step_size: Determines the size of a single step taken by the\n",
       "    verlet integrator while computing the trajectory using Hamiltonian\n",
       "    dynamics. If not specified, it will be set to 1.\n",
       ":param bool adapt_step_size: A flag to decide if we want to adapt step_size\n",
       "    during warm-up phase using Dual Averaging scheme.\n",
       ":param bool adapt_mass_matrix: A flag to decide if we want to adapt mass\n",
       "    matrix during warm-up phase using Welford scheme.\n",
       ":param bool full_mass: A flag to decide if mass matrix is dense or diagonal.\n",
       ":param bool use_multinomial_sampling: A flag to decide if we want to sample\n",
       "    candidates along its trajectory using \"multinomial sampling\" or using\n",
       "    \"slice sampling\". Slice sampling is used in the original NUTS paper [1],\n",
       "    while multinomial sampling is suggested in [2]. By default, this flag is\n",
       "    set to True. If it is set to `False`, NUTS uses slice sampling.\n",
       ":param dict transforms: Optional dictionary that specifies a transform\n",
       "    for a sample site with constrained support to unconstrained space. The\n",
       "    transform should be invertible, and implement `log_abs_det_jacobian`.\n",
       "    If not specified and the model has sites with constrained support,\n",
       "    automatic transformations will be applied, as specified in\n",
       "    :mod:`torch.distributions.constraint_registry`.\n",
       ":param int max_plate_nesting: Optional bound on max number of nested\n",
       "    :func:`pyro.plate` contexts. This is required if model contains\n",
       "    discrete sample sites that can be enumerated over in parallel.\n",
       ":param bool jit_compile: Optional parameter denoting whether to use\n",
       "    the PyTorch JIT to trace the log density computation, and use this\n",
       "    optimized executable trace in the integrator.\n",
       ":param dict jit_options: A dictionary contains optional arguments for\n",
       "    :func:`torch.jit.trace` function.\n",
       ":param bool ignore_jit_warnings: Flag to ignore warnings from the JIT\n",
       "    tracer when ``jit_compile=True``. Default is False.\n",
       ":param float target_accept_prob: Target acceptance probability of step size\n",
       "    adaptation scheme. Increasing this value will lead to a smaller step size,\n",
       "    so the sampling will be slower but more robust. Default to 0.8.\n",
       ":param int max_tree_depth: Max depth of the binary tree created during the doubling\n",
       "    scheme of NUTS sampler. Default to 10.\n",
       ":param callable init_strategy: A per-site initialization function.\n",
       "    See :ref:`autoguide-initialization` section for available functions.\n",
       "\n",
       "Example:\n",
       "\n",
       "    >>> true_coefs = torch.tensor([1., 2., 3.])\n",
       "    >>> data = torch.randn(2000, 3)\n",
       "    >>> dim = 3\n",
       "    >>> labels = dist.Bernoulli(logits=(true_coefs * data).sum(-1)).sample()\n",
       "    >>>\n",
       "    >>> def model(data):\n",
       "    ...     coefs_mean = torch.zeros(dim)\n",
       "    ...     coefs = pyro.sample('beta', dist.Normal(coefs_mean, torch.ones(3)))\n",
       "    ...     y = pyro.sample('y', dist.Bernoulli(logits=(coefs * data).sum(-1)), obs=labels)\n",
       "    ...     return y\n",
       "    >>>\n",
       "    >>> nuts_kernel = NUTS(model, adapt_step_size=True)\n",
       "    >>> mcmc = MCMC(nuts_kernel, num_samples=500, warmup_steps=300)\n",
       "    >>> mcmc.run(data)\n",
       "    >>> mcmc.get_samples()['beta'].mean(0)  # doctest: +SKIP\n",
       "    tensor([ 0.9221,  1.9464,  2.9228])\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/syssynbio/lib/python3.7/site-packages/pyro/infer/mcmc/nuts.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mMCMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minitial_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhook_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmp_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdisable_progbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdisable_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Wrapper class for Markov Chain Monte Carlo algorithms. Specific MCMC algorithms\n",
       "are TraceKernel instances and need to be supplied as a ``kernel`` argument\n",
       "to the constructor.\n",
       "\n",
       ".. note:: The case of `num_chains > 1` uses python multiprocessing to\n",
       "    run parallel chains in multiple processes. This goes with the usual\n",
       "    caveats around multiprocessing in python, e.g. the model used to\n",
       "    initialize the ``kernel`` must be serializable via `pickle`, and the\n",
       "    performance / constraints will be platform dependent (e.g. only\n",
       "    the \"spawn\" context is available in Windows). This has also not\n",
       "    been extensively tested on the Windows platform.\n",
       "\n",
       ":param kernel: An instance of the ``TraceKernel`` class, which when\n",
       "    given an execution trace returns another sample trace from the target\n",
       "    (posterior) distribution.\n",
       ":param int num_samples: The number of samples that need to be generated,\n",
       "    excluding the samples discarded during the warmup phase.\n",
       ":param int warmup_steps: Number of warmup iterations. The samples generated\n",
       "    during the warmup phase are discarded. If not provided, default is\n",
       "    is the same as `num_samples`.\n",
       ":param int num_chains: Number of MCMC chains to run in parallel. Depending on\n",
       "    whether `num_chains` is 1 or more than 1, this class internally dispatches\n",
       "    to either `_UnarySampler` or `_MultiSampler`.\n",
       ":param dict initial_params: dict containing initial tensors in unconstrained\n",
       "    space to initiate the markov chain. The leading dimension's size must match\n",
       "    that of `num_chains`. If not specified, parameter values will be sampled from\n",
       "    the prior.\n",
       ":param hook_fn: Python callable that takes in `(kernel, samples, stage, i)`\n",
       "    as arguments. stage is either `sample` or `warmup` and i refers to the\n",
       "    i'th sample for the given stage. This can be used to implement additional\n",
       "    logging, or more generally, run arbitrary code per generated sample.\n",
       ":param str mp_context: Multiprocessing context to use when `num_chains > 1`.\n",
       "    Only applicable for Python 3.5 and above. Use `mp_context=\"spawn\"` for\n",
       "    CUDA.\n",
       ":param bool disable_progbar: Disable progress bar and diagnostics update.\n",
       ":param bool disable_validation: Disables distribution validation check.\n",
       "    Defaults to ``True``, disabling validation, since divergent transitions\n",
       "    will lead to exceptions. Switch to ``False`` to enable validation, or\n",
       "    to ``None`` to preserve existing global values.\n",
       ":param dict transforms: dictionary that specifies a transform for a sample site\n",
       "    with constrained support to unconstrained space.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/syssynbio/lib/python3.7/site-packages/pyro/infer/mcmc/api.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
